---
title: "Tree-based Methods" 
subtitle: Machine Learning in Economics
author: 
  name: "Prof. Dr. Hüseyin Taştan"
  affiliation: "Yildiz Technical University"
date: "Spring 2022"
output:
  html_document: 
    number_sections: true
    theme: lumen
    highlight: haddock 
    # code_folding: show
    toc: yes
    toc_depth: 3
    toc_float: yes
    keep_md: false


---
<style type="text/css"> 
body{
  background-color: #FAFAFA;
  font-size: 18px;
  line-height: 1.8;
}
code.r{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = TRUE, results = 'asis', fig.show = 'asis')
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
``` 

<br/>
<br/>


# Classification Trees
 
We use `tree` package to grow classification and regression trees. This example uses `Carseats` data set. Sales is recorded as a categorical variable using 8 as the cutoff point. 
```{r}
library(tree)
library(ISLR)
Carseats$High <- factor(ifelse(Carseats$Sales<=8,"No","Yes")) 
tree.carseats <- tree(High ~ . -Sales, Carseats)
summary(tree.carseats) 
```

The training error rate is 9%. 

```{r}
plot(tree.carseats)
text(tree.carseats,pretty=0)
```

The most important indicator of Sales appears to be shelving location, 
since the first branch differentiates Good locations from Bad and Medium
locations.

To see the details of the tree: 
```{r}
tree.carseats
```

In the output above, `R` displays the split criterion (e.g. `Price < 92.5`), the
number of observations in that branch, the deviance, the overall prediction
for the branch (Yes or No), and the fraction of observations in that branch
that take on values of Yes and No. Branches that lead to terminal nodes are
indicated using asterisks.

Computing the test error: 
```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train,]
High.test <- Carseats$High[-train]
tree.carseats <- tree(High ~ . -Sales, Carseats, subset=train)
tree.pred <- predict(tree.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
```
```{r}
# correct prediction rate
(104+50)/200
```

Pruning the tree using cross-validation using `cv.tree()` function: 
```{r} 
set.seed(11)
cv.carseats <- cv.tree(tree.carseats, FUN=prune.misclass)
names(cv.carseats)
cv.carseats
```

In the output above, `dev` is the cross-validation error and `k` is the tuning parameter $\alpha$ and `size` is the number of terminal nodes (tree size). The minimum cv error is 74 and achieved by the model with 8 terminal nodes. 

```{r}
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
```

We now apply the `prune.misclass()` function in order to prune the tree
obtain the 8-node tree.

```{r}
prune.carseats <- prune.misclass(tree.carseats, best=8)
plot(prune.carseats)
text(prune.carseats,pretty=0)
```

In the text book, a 9-node tree is fitted instead: 
```{r}
prune.carseats <- prune.misclass(tree.carseats, best=9)
plot(prune.carseats)
text(prune.carseats,pretty=0)
```

How well does this pruned tree perform on the test data set? Once again,
we apply the `predict()` function.

```{r}
tree.pred <- predict(prune.carseats, Carseats.test, type="class")
table(tree.pred,High.test)
(97+58)/200
```

# Regression Trees

We fit a regression tree to the Boston data set. First, we create a
training set, and fit the tree to the training data.

```{r}
library(MASS)
set.seed(81)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston <- tree(medv ~ ., Boston, subset=train)
summary(tree.boston)
```

Note that only 4 variables are used to construct the tree: "rm" "lstat" "crim"  "age"  

```{r}
plot(tree.boston)
text(tree.boston,pretty=0)
```

The variable `lstat` measures the percentage of individuals with lower
socioeconomic status. The tree indicates that lower values of `lstat` correspond
to more expensive houses.


Pruning the tree: 
```{r}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type='b')
```

In this case, the most complex tree is selected by cross-validation.
However, if we wish to prune the tree, we could do so as follows, using the
`prune.tree()` function:

```{r}
prune.boston <- prune.tree(tree.boston, best=5)
plot(prune.boston)
text(prune.boston,pretty=0)
```

In keeping with the cross-validation results, we use the unpruned tree to
make predictions on the test set.

```{r}
yhat <- predict(tree.boston,newdata=Boston[-train,])
boston.test <- Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
```

The test set MSE associated with the regression tree is 18.93.
The square root of the MSE is therefore around 4.351, indicating
that this model leads to test predictions that are within around $4351 of
the true median home value for the suburb.

# Bagging and Random Forests 

Here we apply bagging and random forests to the `Boston`
data, using the `randomForest` package in R.

Recall that bagging is simply a special case of
a random forest with m = p. Therefore, the `randomForest()` function can
be used to perform both random forests and bagging. We perform bagging
as follows:

```{r}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston, 
                           subset = train, mtry=13, importance=TRUE)
bag.boston
```

The argument `mtry=13` indicates that all 13 predictors should be considered
for each split of the tree—in other words, that bagging should be done. How
well does this bagged model perform on the test set

```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
```

Test MSE = 10.13872

We could change
the number of trees grown by `randomForest()` using the `ntree` argument:

```{r}
bag.boston <- randomForest(medv ~ ., data = Boston, 
                           subset = train, mtry = 13, ntree = 25)
yhat.bag <- predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
```

Growing a random forest proceeds in exactly the same way, except that
we use a smaller value of the `mtry` argument. By default, `randomForest()`
uses p/3 variables when building a random forest of regression trees, and
$\sqrt{p}$ variables when building a random forest of classification trees. Here we
use `mtry = 6`.

```{r}
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston, 
                          subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train,])
mean((yhat.rf-boston.test)^2)
```

Random forest yields slight improvement. 

Using the `importance()` function, we can view the importance of each
variable.

```{r}
importance(rf.boston)
```

```{r}
varImpPlot(rf.boston)
```

The results indicate that across all of the trees considered in the random
forest, the wealth level of the community (lstat) and the house size (rm)
are by far the two most important variables.

# Boosting 

Here we use the `gbm` package, and within it the `gbm()` function, to fit boosted 
regression trees to the `Boston` data set. We run `gbm()` with the option
`distribution="gaussian"` since this is a regression problem; if it were a binary
classification problem, we would use `distribution="bernoulli"`. The
argument `n.trees=5000` indicates that we want 5000 trees, and the option
`interaction.depth=4` limits the depth of each tree.

```{r}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train,], distribution = "gaussian", 
                    n.trees = 5000, interaction.depth = 4)
summary(boost.boston)
```

The `summary()` function produces a relative influence plot and also outputs
the relative influence statistics.

We see that lstat and rm are by far the most important variables. We can
also produce partial dependence plots for these two variables. These plots 
illustrate the marginal effect of the selected variables on the response after
integrating out the other variables. In this case, as we might expect, median
house prices are increasing with rm and decreasing with lstat.

```{r}
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
```

We now use the boosted model to predict medv on the test set:

```{r}
yhat.boost <- predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
```

The test MSE obtained is 11.8; similar to the test MSE for random forests
and superior to that for bagging. If we want to, we can perform boosting
with a different value of the shrinkage parameter λ in (8.10). The default
value is 0.001, but this is easily modified. Here we take λ = 0.2.


```{r}
boost.boston <- gbm(medv~.,data=Boston[train,],
                    distribution="gaussian",n.trees=5000,
                    interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost <- predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)

```

The MSE with λ = 0.2 is higher. 

# Using Tidymodels 

We continue using `Boston` data set to illustrate the usage of Tidymodels tools to estimate regression trees. 

First, load the packages and determine train and test sets using `medv` as the stratification variable. 
```{r}
library(MASS)
library(tidyverse)
library(tidymodels)

# determine training and test sets
# 75% train, 25% test, with stratified sampling based on medv

set.seed(135)
boston_split <- initial_split(Boston, prop=0.75, strata=medv)

boston_train <- boston_split %>% training()

boston_test <- boston_split %>% testing()
```

Then set the engine and the tuning parameters: 
```{r}
# train a decision tree 
dt_model <- decision_tree() %>%
  set_engine('rpart') %>%
  set_mode('regression')

# lets do some parameter tuning 
dt_tune_model <- decision_tree(cost_complexity = tune(),
                               tree_depth = tune(),
                               min_n = tune()) %>%
  set_engine('rpart') %>%
  set_mode('regression')

dt_tune_model
```


Let's create a grid of hyperparameters:
```{r}
# create a grid for hyperparameters
set.seed(12345)
dt_grid <- grid_random(parameters(dt_tune_model), size = 5)
dt_grid
```


Now we can use 5-fold cross-validation to select the best hyperparameters. 
```{r}
# determine CV folds
set.seed(12345)
boston_folds <- vfold_cv(boston_train,
                        v = 5,
                        strata = medv)
boston_folds
```


Define a recipe for the regression tree: 
```{r}
# use all variables without any transformation
boston_recipe <- recipe(medv ~ ., data = boston_train)
boston_recipe
```


```{r}
# create workflow 
boston_wkfl <- workflow() %>%
  add_model(dt_tune_model) %>%
  add_recipe(boston_recipe)
```


```{r}
# apply decision tree algorithm to all CV folds
dt_tuning <- boston_wkfl %>%
  tune_grid(resamples = boston_folds,
            grid = dt_grid)

dt_tuning %>% collect_metrics()
```


```{r}
# detailed information on hyperparameters
dt_tuning %>%
  collect_metrics(summarize = FALSE) %>%
  filter(.metric == 'rmse') %>%
  group_by(id) %>%
  summarize(min_rmse = min(.estimate),
            median_rmse = median(.estimate),
            max_rmse = max(.estimate))
```


```{r}
# which one is the best?
dt_tuning %>%
  show_best(metric = 'rmse', n = 5)
```


```{r}
dt_tuning %>%
  show_best(metric = 'rsq', n = 5)
```


```{r}
best_dt_model <- dt_tuning %>%
  select_best(metric = 'rmse')
best_dt_model
```


```{r}
# finalize the workflow (fixes the hyperparameters)
final_boston_wkfl <- boston_wkfl %>%
  finalize_workflow(best_dt_model)
final_boston_wkfl
```

Finally, obtain the test RMSE: 
```{r}
# train the decision tree 
boston_final_fit <- final_boston_wkfl %>%
  last_fit(split = boston_split)
boston_final_fit %>%
  collect_metrics()
```


Now, let's fit a **random forest** model using Tidymodels. 
```{r}
# determine the number of cores for parallel computing
cores <- parallel::detectCores()
```


```{r}
# set the engine and mode, also the tuning parameters
# mtry and min_n are set to be determined by CV
rf_model <- rand_forest() %>%
  set_args(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode('regression')
```


```{r}
# use all variables without any transformation
boston_recipe <- recipe(medv ~ ., data = boston_train) 
# create a random forest workflow
rf_workflow <- workflow() %>% 
  add_recipe(boston_recipe) %>% 
  add_model(rf_model)
```


```{r}
# perform cross-validation on a grid of hyperparameters
# boston_folds was created above
set.seed(255)
rf_tune_results <- rf_workflow %>% 
  tune_grid(resamples = boston_folds,  # CV folds
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))

rf_tune_results %>% show_best(metric = "rmse")
# mtry = 7 min_n=3
# plot hyperparameter values 
autoplot(rf_tune_results)

rf_tune_results %>% collect_metrics()
```
```{r}
best_rf_model <- rf_tune_results %>%
  select_best(metric = 'rmse')
best_rf_model
```
```{r}
final_rf_wkfl <- rf_workflow %>%
  finalize_workflow(best_rf_model)
final_rf_wkfl

rf_final_fit <- final_rf_wkfl %>%
  last_fit(split = boston_split)

rf_final_fit %>%
  collect_metrics()
```


```{r}
# last fit 
last_rf_model <- 
  rand_forest(mtry = 7, min_n = 3, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("regression")

last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_model)

set.seed(678)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(boston_split)

last_rf_fit

last_rf_fit %>% collect_metrics()
```


```{r}
# variable importance measures
library(vip) # for variable importance measures
last_rf_fit %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 13)
```




# Example: Surviving the Titanic 

In this exercise, we will attempt to predict who will survive in the Titanic accident which is one of the worst disasters at sea. On 14 April 1912, the [RMS Titanic](https://en.wikipedia.org/wiki/RMS_Titanic) struck a large iceberg and approximately 1,500 passengers and crew died and 705 survived on 15 April. The Titanic disaster has led to significant improvements in safety regulations to prevent such calamities happening again. 

We are interested in the following questions: What are the factors behind disproportionate number of deaths in this disaster?
Which variables are relatively more important that influenced the likelihood of survival?
We will start by building a classification tree to answer these questions. 

First, read Titanic data into R: 
```{r}
library(tidyverse)
library(modelr)
library(broom)
set.seed(1234)

theme_set(theme_minimal())

library(titanic)
# use the training data 
titanic <- titanic_train %>%
  as_tibble()

titanic %>%
  head() %>%
  knitr::kable()
```


```{r}
titanic_tree_data <- titanic %>%
  mutate(Survived = if_else(Survived == 1, "Survived", "Died"),
         Survived = as.factor(Survived),
         Sex = as.factor(Sex))
titanic_tree_data
table(titanic_tree_data$Survived)
```

Construct a simple classification tree using only two features, age and sex, using `partykit` package.

```{r}
library(partykit)
titanic_tree <- ctree(Survived ~ Age + Sex, data = titanic_tree_data)
titanic_tree
```

The resulting tree has 3 terminal nodes and 2 inner nodes. Here is the 
plot of the tree 
```{r}
plot(titanic_tree)
```

Compute misclassification rate: 
```{r}
# compute predicted values
pred1 <- predict(titanic_tree) 
# true classifications
predsuccess1 <- (pred1 == titanic_tree_data$Survived)
# wrong classifications
prederror1 <- !predsuccess1
# average misclassification rate: 
mean(prederror1, na.rm = TRUE)

# or just in one line: 
# 1 - mean(predict(titanic_tree) == titanic_tree_data$Survived, na.rm = TRUE)
```

Even with this simple tree, the error rate is 21%. 

Let's create a more complicated tree using additional features. 
First convert character variables into factor variables. 

```{r}
titanic_tree_full_data <- titanic %>%
  mutate(Survived = if_else(Survived == 1, "Survived",
                            if_else(Survived == 0, "Died", NA_character_))) %>%
  mutate_if(is.character, as.factor)

```

Construct the tree: 
```{r}

titanic_tree_full <- ctree(Survived ~ Pclass + Sex + Age + SibSp +
                             Parch + Fare + Embarked,
                           data = titanic_tree_full_data)
titanic_tree_full

```

```{r}
plot(titanic_tree_full,  ip_args = list(pval = FALSE, id = FALSE),
     tp_args = list( id = FALSE) )
```

```{r}
# error rate
1 - mean(predict(titanic_tree_full) == titanic_tree_data$Survived,
         na.rm = TRUE)
```

Let's build a random forest using `caret` package. We will use `caret::train()` function and plot the most important variables. 
```{r}
# drop the NA obs.
titanic_rf_data <- titanic_tree_full_data %>%
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
  drop_na()
titanic_rf_data

```

```{r, warning=FALSE}
# build a random forest with ntree=200
library(caret)
library(e1071)
titanic_rf <- train(Survived ~ ., data = titanic_rf_data,
                    method = "rf",
                    ntree = 200,
                    trControl = trainControl(method = "oob"))
titanic_rf
```



```{r}
str(titanic_rf, max.level = 1)
# Info on the final model: 
titanic_rf$finalModel
```



```{r}
# confusion table 
knitr::kable(titanic_rf$finalModel$confusion)
```



```{r}
# look at an indiividual tree
randomForest::getTree(titanic_rf$finalModel, labelVar = TRUE)
```



```{r}
# var importance plot 
randomForest::varImpPlot(titanic_rf$finalModel)
```



<br>
<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>


